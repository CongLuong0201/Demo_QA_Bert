{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR4YqkIYKPZR",
        "outputId": "52d6201c-e309-4390-f7be-1f754091aeb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "OnW53GzOKYDg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "7IiC22tEKa3Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "0PJKJisYKfCD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Lấy input là chuỗi string của câu question và answer_text chứa nội dung câu trả lời của câu question.\n",
        "    Xác định từ trong answer_text là câu trả lời và in ra.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Áp dụng tokenizer cho cặp câu <question, answer_text>. input_ids là concatenate indice của cả 2 câu sau khi đã thêm các token CLS và SEP như mô tả trong tác vụ Question and Answering.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Xác định vị trí đầu tiên chứa token [SEP] trong câu.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "   # Tạo segment index đánh dấu các vị trí từ thuộc question (giá trị 0) và answer_text (giá trị 1)\n",
        "    num_seg_a = sep_index + 1\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "     # Kiểm tra độ dài segment_ids phải bằng input_ids\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Dự báo phân phối xác suất của vị trí của từ start và từ end trong chuỗi concatenate <question, answer_text> mà chứa kết quả cho câu trả lời.\n",
        "    outputs = model(torch.tensor([input_ids]), \n",
        "                    token_type_ids=torch.tensor([segment_ids]), \n",
        "                    return_dict=True) \n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Tìm ra vị trí start, end với score là cao nhất\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Chuyển ngược từ input_ids sang list tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Token đầu tiên của câu trả lời\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Lựa chọn các thành phần còn lại của câu trả lời và join chúng với whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # Nếu token là một subword token (có dấu ## ở đầu) thì combine vào answer bằng token gốc (loại bỏ dấu ##).\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Nếu trái lại thì combine trực tiếp vào answer.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "metadata": {
        "id": "PXYo5_hNLlec"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Bọc văn bản thành 80 ký tự.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLbuvd1SK7OG",
        "outputId": "21239f95-fda3-4531-c951-4e067dcd6437"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We introduce a new language representation model called BERT, which stands for\n",
            "Bidirectional Encoder Representations from Transformers. Unlike recent language\n",
            "representation models (Peters et al., 2018a; Radford et al., 2018), BERT is\n",
            "designed to pretrain deep bidirectional representations from unlabeled text by\n",
            "jointly conditioning on both left and right context in all layers. As a result,\n",
            "the pre-trained BERT model can be finetuned with just one additional output\n",
            "layer to create state-of-the-art models for a wide range of tasks, such as\n",
            "question answering and language inference, without substantial taskspecific\n",
            "architecture modifications. BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art results on eleven natural language\n",
            "processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute\n",
            "improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1\n",
            "question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD\n",
            "v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What does the 'B' in BERT stand for?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm4z11RBLBNm",
        "outputId": "c5caab78-6b49-42ea-82a6-547f0a188acf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 258 tokens.\n",
            "\n",
            "Answer: \"bidirectional encoder representations from transformers\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is my dog name?\"\n",
        "paragraph = \"I have a dog. It's name is Ricky. I get it at my 15th birthday, when it was a puppy.\"\n",
        "\n",
        "answer_question(question, paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7cuOZ5wM2Hm",
        "outputId": "0f8a82c8-21f3-49a6-8862-c98c2c1d85e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 35 tokens.\n",
            "\n",
            "Answer: \"ricky\"\n"
          ]
        }
      ]
    }
  ]
}